# 低照度增强
## 基于直方图的增强方法
### 全局直方图1
图像光照不均匀，效果不好 , 由于是全局图像增强，效果不可控，不能突出图像中的目标信息
### 局部直方图均衡化
仅考虑局部窗口内的灰度分布,没有考虑图像整体特点，易减弱图像的层次感
## 基于同态滤波的增强方法
基于照明-反射模型，
- 在对数域，将乘性的照明与反射分量变成加性，
- 在傅里叶变换域，由高通滤波器增强高频的反射分量，而抑制低频的照明分量。
该方法主要存在的两个问题：
1. 若高通滤波器的截止频率过高，会导致动态范围严重压缩以及细节严重丢失；反之，则动态范围压缩下降，即缺乏自适应性。
2. 因为该方法的前提条件是假设光照均匀，所以对于存在高光区和暗区的夜间图像，增强效果较差。
## 基于Retinex理论的增强方法
其基本思想是在对数域，设法估计照明分量并去除得到反映图像内容的反射分量，再对其进行一定的对比度拉伸处理。
算法的关键是如何估计照射光成分，一般使用高斯滤波器估计照射分量。
单尺度Retinex(SSR)只对原始图像进行滤波估计，若采用多尺度Retinex(MSR)，则要在图像的多个尺度下用高斯函数卷积求照射分量，然后加权求和。
该类算法存在的主要问题包括：
1. MSR耗时严重，且在图像边缘区，存在光晕问题；
2. 照射分量难以准确估计，故对于存在高光区和暗区的低照度图像，增强效果不佳。

# 伽玛变换
伽玛变换（Gamma Transformation），也常被称为**伽玛校正** 或**幂律变换**，是一种非常简单却极其重要的**非线性**点运算（即每个像素点的输出值只取决于其自身的输入值）。
它的核心是一个数学公式：
**s = c × r^γ**
其中：
*   **r** 是原始像素的归一化亮度值（通常范围是 [0, 1]，0代表最暗/黑色，1代表最亮/白色）。
*   **γ** 就是**伽玛值**，是整个变换的核心参数。
*   **c** 通常是一个常数，为了简单起见，常设为1。
*   **s** 是变换后像素的亮度值。
所以，简化后的公式就是：**输出 = 输入^γ**
**关键点在于：伽玛变换不是线性地拉伸或压缩亮度，而是有选择地对暗部区域或亮部区域的亮度进行非均匀调整。**

| 放大率                                | 折射球面             |
| :--------------------------------- | :--------------- |
| $\beta=y' /y$                      | $-x' /f'=- f /x$ |
| $\alpha=\mathrm{d}l' /\mathrm{d}l$ | $-x' /x$         |
| $\gamma=u' /u$                     | $x /f' =f /x'$   |


| 特性        | 描述                            |
| :-------- | :---------------------------- |
| **本质**    | 一种非线性的亮度变换：`输出 = 输入^γ`        |
| **γ > 1** | 图像变暗，压缩暗部细节，增强亮部对比度。          |
| **γ < 1** | 图像变亮，增强暗部细节，压缩亮部对比度。          |
| **γ = 1** | 图像无变化。                        |
| **主要用途1** | **伽玛校正**：校正显示系统的非线性，保证图像正确显示。 |
| **主要用途2** | **图像增强**：有选择地调整对比度，突出图像细节。    |
# 直方图均衡化
直方图均衡化的核心思想是：**将原始图像的灰度直方图——即像素亮度值的分布——从可能比较集中的某个区间，重新均匀地分布到整个可能的亮度范围内，从而增强图像的对比度。**
对于低照度图像来说，它的像素值通常大量集中在暗部区域（直方图左侧），导致图像昏暗、细节不清。均衡化的目的就是“拉平”这个直方图，把暗部的像素“拉开”，让暗部细节显现出来。
在理解均衡化之前，必须先明白**图像直方图**是什么。
*   **定义**：图像直方图是一个图表，它统计了一幅图像中**每个亮度级别（从0到255，0为黑，255为白）的像素数量**。
*   **低照度图像的直方图特征**：
    *   像素大量堆积在左侧（暗调区域）。
    *   中间调和亮调区域的像素非常少，甚至没有。
    *   这直接导致了图像看起来昏暗、缺乏层次感。*
---
它的目标是为图像创建一个**变换函数**，使得输出图像的直方图尽可能平坦（即每个灰度级的像素数量大致相等）。
**步骤可以简化为：**
1.  **统计直方图**：遍历整张图像，计算每个灰度级 \( $r_k$ \)（例如 $k=0,1,2,...,255$）的像素数量 \( $n_k$ \)。
2.  **计算概率分布**：计算每个灰度级出现的概率 \( $p_r(r_k) = n_k / n$ \)，其中 \( $n$ \) 是图像的总像素数。
3.  **计算累积分布函数**：这是最关键的一步。计算累积概率（即从最暗到当前灰度级的所有像素概率之和）：
    \( $S_k = \sum_{j=0}^{k} p_r(r_j)$ \)
    这个 \( S_k \) 的值在 0 到 1 之间。
4.  **将累积分布映射到新的灰度级**：将上一步得到的累积值 \( $S_k$ \) 映射到 0 到 255 的整数范围内（对于8位图像）。新灰度级为：
    \( $s_k = \text{round}(S_k \times 255$) \)
    这里 `round` 表示四舍五入取整。
5.  **生成新图像**：根据这个映射关系，将原图中所有灰度级为 \( $r_k$ \) 的像素，其值全部替换为对应的新灰度级 \( $s_k$ \)。
---
### 切片自适应直方图均衡化
为了解决全局直方图均衡化的缺点（尤其是放大噪声和过度增强），人们提出了更高级的改进算法，其中最著名的是**CLAHE**。
*   **CLAHE**：
    *   **核心思想**：不再对整张图像进行全局均衡化，而是将图像**分割成许多个小块（Tile）**。
    *   **工作流程**：
        1.  对每一个小块独立进行直方图均衡化。
        2.  为了消除块与块之间的边界痕迹，使用**双线性插值** 将相邻小块的像素平滑地融合起来。
        3.  引入**对比度限制**，防止噪声被过度放大。它会设定一个阈值，如果某个灰度级的像素数量超过这个阈值，就将超出的像素“裁剪”掉，并平均分配到整个直方图上，从而抑制噪声的增强。
    *   **优势**：CLAHE 特别适用于低照度图像增强，它能有效地增强局部对比度和细节，同时有效地控制噪声的放大，结果比全局均衡化自然得多。现代很多图像处理软件和库（如OpenCV）的首选就是CLAHE。
---
HE 往往会放大平滑区域的噪点,导致细节丢失;AHE 则会带来伪影和边缘  增强。


# Retinex
Retinex 算法(SSR)仍然是图像增强的主流方法
Retinex理论的核心思想是：**我们人眼所感知到的物体的颜色和亮度，并不完全取决于物体本身的绝对反射光强度，而是取决于物体在不同波长（红、绿、蓝三色）上的反射光在环境光照下的相对关系。**
它的目标是**从一张退化的图像中，分离出代表物体本身属性的“反射分量”，并去除或减轻代表环境光照不均的“光照分量”的影响。**
Retinex 这个词是由 **Retina（视网膜）** 和 **Cortex（大脑皮层）** 组合而成。这表明该理论试图模拟人类视觉系统的工作方式：
1.  **视网膜**负责接收光线。
2.  **大脑皮层**负责处理信息，能够“智能地”排除光照变化的影响，从而稳定地感知物体的本质颜色和亮度。

Retinex算法基于一个简单的成像物理模型：
`I(x, y) = R(x, y) * L(x, y)`
其中：
*   **I(x, y)**：是我们**观测到的图像**（Image），也就是相机直接拍到的图片。
*   **R(x, y)**：是物体的**反射属性分量**（Reflectance）。它代表了物体本身固有的特性，是算法希望得到的理想结果。理论上，**R是“好”的、细节丰富的、光照均匀的图像**。
*   **L(x, y)**：是**入射光照分量**（Illumination）。它代表了环境光线的分布，是造成图像亮度不均、阴影、过曝等问题的根源。
**算法的目标**：已知 **I**，如何估计出 **L** 并进而求解出 **R**？
通常，求解过程是：**R = I / L**。
**难点在于**：如何从混合在一起的 **I** 中，合理地估计出光照分量 **L**？不同的Retinex算法主要区别就在于如何估计 **L**。
### 经典算法流程（以单尺度Retinex为例）
1.  **对数变换**：
    *   将乘法模型转换为加法模型，便于处理。
        `log(I) = log(R * L) = log(R) + log(L)`
    *   现在，目标变成了从 `log(I)` 中分离出 `log(R)` 和 `log(L)`。
2.  **估计光照分量 L**：
    *   这是最关键的一步。基本假设是：**光照分量 L 是缓慢变化的**，对应图像的低频信息（整体明暗分布）；而**反射分量 R 是急剧变化的**，对应图像的高频信息（物体的边缘、纹理等细节）。
    *   因此，可以通过一个**低通滤波器**（如高斯模糊滤波器）对原始图像 **I** 进行卷积操作，得到的结果就是对光照分量 **L** 的估计。
        `L_estimated = GaussianBlur(I)`
3.  **计算反射分量 R**：
    *   在对数域中，用观测图像减去估计的光照图像：
        `log(R_estimated) = log(I) - log(L_estimated)`
    *   然后通过指数变换换回线性域：
        `R_estimated = exp(log(R_estimated))`
4.  **结果后处理**：
    *   计算出的 `R_estimated` 通常动态范围很宽，且可能不在标准的 [0, 255] 范围内。因此需要进行**对比度拉伸**或**归一化**，将其调整到适合显示的范围内。
### 衍生
根据估计光照分量 **L** 方式的不同，衍生出了多种Retinex算法：

| 算法名称                        | 核心思想                                           | 优点                                                    | 缺点                       |
| :-------------------------- | :--------------------------------------------- | :---------------------------------------------------- | :----------------------- |
| **SSR（单尺度Retinex）**         | 使用**一个固定尺度（标准差）的高斯模糊**来估计L。                    | 实现简单，能一定程度平衡动态范围。                                     | 效果严重依赖尺度参数选择。可能产生“光晕”伪影。 |
| **MSR（多尺度Retinex）**         | 使用**多个不同尺度的高斯模糊**分别估计L，然后将结果加权融合。              | 结合了大尺度（保持颜色恒常性）、中尺度（增强对比度）和小尺度（保留细节）的优点，效果比SSR更好、更鲁棒。 | 计算量增大，仍可能有一定光晕。          |
| **MSRCR（带颜色恢复的多尺度Retinex）** | 在MSR的基础上，增加了**颜色恢复因子**。因为MSR/MSR在处理后可能会导致颜色失真。 | **最经典、最常用的版本**。能很好地增强图像的同时，保持颜色的自然和真实性。               | 参数较多，需要调优。               |


| 优点                        | 缺点                                  |
| :------------------------ | :---------------------------------- |
| **理论基础坚实**，模仿人类视觉，物理意义清晰。 | **计算复杂度相对较高**，尤其是MSR/MSRCR。         |
| **增强效果自然**，能同时处理全局和局部对比度。 | 可能产生**“光晕”伪影**，在强边缘附近出现亮或暗的带状区域。    |
| **能有效保持边缘和细节**。           | 对**噪声比较敏感**，增强图像的同时也可能放大噪声。         |
| **应用范围广泛**，适用于多种图像退化模型。   | 参数（如高斯尺度、权重）需要**谨慎调优**，不同图像可能需不同参数。 |
# 对Retinex的改进
占必超等人设一种基于平稳小波变换和 Retinex 的红外图像增强方法,改善了图像的整体视觉效果。
Zotin提出基于 HSV 颜色模型的多尺度 Retinex 快速图像增强算法,通过在 MSR 中使用  HSV 模型的 V 通道进行亮度校正。
Pu 等人提出了一种分数阶 Retinex(FR)方法,用于自适 应增强曝光不足的交通图像的对比度。
# 深度学习的方法
Lore 等人首次提出的 LLNet 神经网络能够提升图像亮度,但会产生明显伪影。
Luan 等人推出的 FDGN 网络通过特征蒸馏和引导机制提高图像质量,有效纠  正色彩偏差并增强细节,但在极端黑暗等复杂场景中表现欠佳,可能产生伪影或过度增强。
马红强等人提出的基于 DCNN 的算法适用于非均匀光照条件,但受限于网络深度和宽度,  仅适用于特定场景。
Pei 等人提出了一种基于变压器架构的 U-Net 架构 FFTFormer,该架  构集成了自注意机制(CA_Swin)、FFT 嵌入式融合模块(FFTF)和 CNN 解码器,以获得  更好的图像增强效果。
## LLNet
(Low Light Network，低光照网络)
- LLNet 的核心思想是**堆栈式稀疏去噪自编码器**。
- **自编码器**：是一种无监督神经网络，目标是将输入数据（如图像）压缩成一个低维的“编码”，然后再从这个编码中尽可能地**重建**出原始输入。通过这个“压缩-重建”的过程，网络可以学习到数据中最重要、最本质的特征。
- **去噪自编码器**：是自编码器的一个变种。训练时，我们向清晰的输入图像中添加噪声，然后让网络学习如何从**带噪声的图像**中重建出**清晰的原始图像**。这样，网络就学会了去噪的能力。
- **堆栈式**：指将多个去噪自编码器层层堆叠起来，形成一个更深的网络。浅层网络可能学习去除简单的噪声，而更深层的网络可以处理更复杂的图像退化问题（如亮度增强和对比度恢复）。
- **稀疏性**：在训练过程中对神经元的激活加以限制，让大部分神经元处于“休眠”状态，只有少数对当前输入敏感的神经元被激活。这有助于网络学习到更鲁棒、更具代表性的特征，防止过拟合。
## FDGN
（Feature Decomposition Guided Network，特征分解引导网络）
- 其核心创新在于 **“特征分解”** 。FDGN 认为，一个破损图像的特征应该被分解成两个部分：
    1. **结构特征**：描述图像中物体的轮廓、形状、几何布局等宏观信息。
    2. **纹理特征**：描述物体表面的细节、图案、颜色等微观信息。
- **双通路网络设计**：FDGN 通常包含两个并行的子网络或通路：
    - **结构通路**：专注于从已知区域推断并生成缺失区域的合理**结构**。
    - **纹理通路**：专注于根据生成的结构，填充逼真的**纹理**细节。
- **引导机制**：两个通路不是独立的。结构通路生成的结果会作为“引导”或“蓝图”，告诉纹理通路“应该在哪里填充什么样的纹理”，从而确保修复后的区域在结构和纹理上都与周围环境协调一致。
## U-Net
**图像分割**，特别是**生物医学图像分割**。它的目标是给图像中的每个像素分配一个标签，从而区分出不同的物体或区域（例如，在医学CT图像中分割出肿瘤区域）。
- **编码器-解码器结构**：
    - **编码器（下采样路径，U-Net的左半部分）**：由一系列卷积层和池化层组成，逐步提取图像特征，同时降低图像分辨率（空间尺寸变小，通道数变多）。这个过程是为了理解图像中的“是什么”（语义信息）。
    - **解码器（上采样路径，U-Net的右半部分）**：由一系列上采样层和卷积层组成，逐步将低分辨率特征图恢复至高分辨率。这个过程是为了确定物体“在哪里”（定位信息）。
- **跳跃连接**：这是U-Net最核心、最创新的设计！它通过将**编码器**中同层级的**高分辨率特征图**与**解码器**中对应的**上采样特征图**进行通道合并。
    - **作用**：编码器的高分辨率特征图提供了丰富的细节和位置信息，而解码器的特征图提供了高级的语义信息。跳跃连接将这两者结合，使得解码器在恢复分辨率时，既能“知道”要分割什么（来自解码路径），又能“知道”精确的边界在哪里（来自编码路径），从而实现了精准的像素级定位。


# RRM递归残差模块
递归残差模块（Recursive Residual Module, RRM）是残差学习（Residual Learning）的一种变体，常用于深度神经网络（尤其是图像增强/重建类任务），能缓解梯度消失问题
## 梯度消失问题的本质
在一个普通的深层前馈网络里，反向传播时梯度是逐层通过链式法则相乘传播的：
$$\frac{\partial L}{\partial x_0}  = \frac{\partial L}{\partial x_n} \prod_{i=1}^n \frac{\partial x_i}{\partial x_{i-1}}$$

如果每一层的梯度因子 ($\frac{\partial x_i}{\partial x_{i-1}}$) 的模都 < 1，那么乘积就会指数级衰减，导致梯度在前层几乎消失。
## 残差连接的作用
ResNet 提出的 **残差结构**：
$$y = F(x) + x$$
在反向传播时，梯度可以绕过 ($F(x)$) 直接传递：
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\left(\frac{\partial F}{\partial x} + I\right)$$
其中的 **恒等映射项 (I)** 确保即使 ($\frac{\partial F}{\partial x}$) 很小，梯度也至少能通过 (I) 保持不衰减。  
👉 这就是残差结构缓解梯度消失的核心。
## 递归残差模块（RRM）的进一步改进
RRM 把同一个残差模块 **递归共享参数地多次迭代应用**，形式大致如下：
$h^{(t+1)} = F(h^{(t)}) + h^{(t)}, \quad t=0,1,2,\dots,T$  
- **多次递归叠加**：通过残差连接保证每次迭代的梯度都能绕过 (F)，避免梯度消失。
- **参数共享**：避免了参数过多引起的训练困难，同时在梯度传播时保持一致的路径。
- **等价于动态展开的深层残差网络**：虽然网络递归展开后很深，但因为有残差路径，梯度仍能顺畅传播。
在反向传播时，梯度流不仅通过每一步的 (F)，还通过所有的恒等映射路径聚合，形成多条“捷径”，显著减弱梯度消失风险。

好的，我们来详细介绍一下这三个在图像质量评估中至关重要的指标：PSNR、SSIM和LPIPS。它们代表了从传统信号保真度到人类视觉感知，再到深度学习感知相似度的演进。
# 特殊卷积操作
## Space-to-Depth空间到深度卷积
`SPD-Conv (Space to Depth Convolution)` 本质上就是把输入特征图的空间维度重新排列到通道维度，再进行卷积。这个思路在超分辨率、轻量化网络（比如 ESPNet、ShuffleNet 变种）里常见。
假设输入张量维度是：  
$$(N, C, H, W)  $$
进行 Space-to-Depth，block_size = $r$，得到输出：  
$$(N, C \cdot r^2, H/r, W/r)  $$
即：每个 ($r \times r$) 的小 patch 被压缩到通道维度。

PyTorch 内置了 `nn.PixelUnshuffle`，正好对应 **space-to-depth**。
```python
import torch
import torch.nn as nn

class SPDConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, block_size=2):
        super(SPDConv, self).__init__()
        self.space_to_depth = nn.PixelUnshuffle(block_size)
        self.conv = nn.Conv2d(in_channels * (block_size ** 2),
                              out_channels,
                              kernel_size,
                              stride=1,
                              padding=kernel_size // 2)

    def forward(self, x):
        x = self.space_to_depth(x)  # (N, C*r^2, H/r, W/r)
        x = self.conv(x)
        return x

# 测试
inp = torch.randn(1, 3, 32, 32)  # batch=1, C=3, H=W=32
model = SPDConv(3, 64, block_size=2)
out = model(inp)
print(out.shape)  # torch.Size([1, 64, 16, 16])
```
## Dilated Conv扩张卷积
又称Atrous卷积
在卷积神经网络中，每个卷积层的一个神经元也只能“看到”输入图像的一小部分，这部分的大小就是它的**感受野**。传统的卷积操作（比如3x3卷积）的感受野是固定的、局部的。
**扩张卷积要解决的核心问题是：如何在不增加计算量（参数数量）和不进行下采样（不丢失信息）的情况下，快速增大神经元的感受野？**
扩张卷积，有时也形象地称为**空洞卷积**。它与标准卷积的唯一区别在于**卷积核的间距**。
*   **标准卷积**：卷积核的每个点都是紧密相邻的。例如，一个3x3的卷积核会检查输入图像上9个连续像素。
*   **扩张卷积**：在卷积核的元素之间**插入空洞（间隔）**。这个间隔的大小由一个超参数 **扩张率** 来控制。
**扩张率** 定义了卷积核处理数据时各值之间的间距。例如：
*   **扩张率 = 1**：这就是标准卷积，没有间隔。
*   **扩张率 = 2**：这意味着在卷积核的每个元素之间插入1个像素的间隔。一个3x3的卷积核，其有效感受野会扩大为5x5，但**实际参与计算的权重仍然只有9个**。
*   **扩张率 = 3**：间隔为2个像素，3x3卷积核的有效感受野会扩大为7x7。
下图直观地展示了不同扩张率下，一个3x3卷积核（红色点）的感受野（蓝色区域）如何变化：
扩张卷积的优势非常明显：
1.  **指数级扩大感受野**：通过增加扩张率，可以以指数方式快速增大感受野，让神经元即使在网络的较浅层也能捕获更大范围的上下文信息。
2.  **保持高分辨率**：与使用池化层或步长卷积进行下采样不同，扩张卷积是在**不降低特征图空间尺寸**的前提下增大感受野的。这对于需要精确定位的任务（如图像分割、图像增强、细节恢复）至关重要，因为它避免了信息丢失。
3.  **参数效率高**：感受野变大了，但**卷积核的权重数量并没有增加**。一个3x3的扩张卷积，无论扩张率多大，都只有9个参数。这使得它在计算上非常高效。
扩张卷积并非完美。当堆叠多个高扩张率的卷积层时，可能会出现**网格效应**：卷积核的感知区域可能无法覆盖所有像素，导致某些信息被完全忽略，就像一张渔网，网眼太大就会漏掉小鱼。
解决方案通常是使用**混合扩张率**（如 [1, 2, 5, 1, 2, 5]）或更复杂的结构（如“锯齿”模式），以确保感受野能覆盖所有区域而没有空洞。

| 特性       | 标准卷积    | 池化/步长卷积           | 扩张卷积              |
| :------- | :------ | :---------------- | :---------------- |
| **主要目的** | 提取局部特征  | 降低分辨率、增大感受野       | **增大感受野**         |
| **如何实现** | 密集扫描    | 选择（池化）或跳过（步长）像素   | **在卷积核元素间插入间隔**   |
| **感受野**  | 小（如3x3） | 增大，但**以损失分辨率为代价** | **显著增大，且保持分辨率不变** |
| **参数数量** | 固定      | 固定                | **固定（与标准卷积相同）**   |
| **关键参数** | 卷积核大小   | 池化大小/步长           | **扩张率**           |

## Stride Conv步幅卷积
就是带步幅（stride）的普通卷积。
- **普通卷积**：卷积核在输入特征图上滑动，每次移动 1 个像素（stride=1），输出特征图大小几乎和输入差不多（取决于 padding）。
- **步幅卷积**：当 stride > 1 时，卷积核每次移动多个像素，相当于做 **下采样**。
**公式：**  
$$\text{output size} = \left\lfloor \frac{(H + 2p - k)}{s} \right\rfloor + 1  $$
其中 ($H$)=输入大小，($k$)=卷积核大小，($p$)=padding，($s$)=stride。
**直观理解**：Stride Conv = 卷积 + 降采样。  
它在分类网络里经常替代 pooling（最大池化/平均池化），比如 ResNet 的前几层。
**例子：**
```python
import torch.nn as nn

# 3x3卷积，stride=2，相当于下采样一半
conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1)
```
输入 (1, 3, 32, 32) → 输出 (1, 64, 16, 16)
## Deconv转置卷积 / 反卷积，Transposed Convolution
常见名字：
- **Deconv**（反卷积，早期叫法）
- **Transposed Convolution**（更准确的称呼）
- **Fractionally Strided Convolution**（分数步幅卷积）
它的主要作用是 **上采样**（把小的特征图变大）。
⚠️ 注意：这里的 "Deconv" 并不是严格意义上的数学反卷积，而是普通卷积的“转置运算”，在实现上相当于在输入元素之间插入 0，再做普通卷积。
**直观理解**：
- Stride Conv：空间变小（下采样）。
- Deconv：空间变大（上采样）。
**例子：**
```python
import torch.nn as nn

# 3x3转置卷积，stride=2，上采样一倍
deconv = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=2, padding=1, output_padding=1)
```
输入 (1, 64, 16, 16) → 输出 (1, 3, 32, 32)
# 图像评估指标
### 一、PSNR（峰值信噪比）
#### 1. 核心思想
PSNR是最广泛使用的**全参考图像质量评估**指标之一。它基于**均方误差**，衡量的是**重建图像（如压缩后、去噪后、超分辨率后的图像）与原始图像之间的像素级误差**。PSNR值越高，代表图像失真越小，质量越好。
#### 2. 计算方法
1.  **计算均方误差**：
    *   对于大小为 \(m \times n\) 的灰度图像，MSE定义为：$$MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1} [I(i, j) - K(i, j)]^2$$
    *   其中，$I$是原始图像，$K$ 是待评估图像。
    *   对于彩色图像，通常计算RGB三个通道的MSE后再平均。
2.  **计算PSNR**：
    *   PSNR（单位：dB）通过对数形式将MSE映射到一个更大的范围：$$PSNR = 10 \cdot \log_{10}\left(\frac{MAX_I^2}{MSE}\right)$$
    *   其中，\($MAX_I$\) 是图像像素点可能的最大值。对于8位图像，这个值是255。
#### 3. 解读与特点
*   **取值范围**：通常在20到40 dB之间。大于30 dB通常认为质量不错，大于40 dB则质量非常高，差异难以察觉。
*   **优点**：
    *   **计算简单，数学意义清晰**，易于实现和理解。
    *   **物理意义明确**：严格衡量像素值的差异。
*   **缺点**：
    *   **与人类主观感知相关性较差**。这是它最致命的弱点。PSNR只关心像素值的差异，而不关心这些差异在视觉上的重要性。
        *   **例子**：一张轻微高斯模糊的图像和一张稍微偏移了几个像素的图像，可能具有相同的PSNR。但人眼会认为模糊的图像质量更差，而偏移在视觉上可能不那么明显。PSNR无法区分这两种情况。
#### 4. 适用场景
*   需要快速、定量比较算法性能的初步评估。
*   在**图像/视频编码**等领域，因为它直接与压缩引入的误差相关。
*   由于其局限性，在学术研究中已逐渐被更先进的指标替代，但在工程中仍很常见。
---
### 二、SSIM（结构相似性指数）
#### 1. 核心思想
SSIM的提出是为了克服PSNR的不足。它认为，人眼视觉系统的主要功能是从视野中提取**结构信息**。因此，它通过比较图像之间的**亮度、对比度和结构**三个方面的相似性来评估质量。
#### 2. 计算方法
SSIM的计算是在图像的小窗口（例如8x8或11x11的滑动窗口）上进行的，最后对所有窗口的SSIM值求平均（即MSSIM）。
对于图像块 \($x$\) 和 \($y$\)，SSIM定义为：
$$SSIM(x, y) = [l(x, y)]^\alpha \cdot [c(x, y)]^\beta \cdot [s(x, y)]^\gamma$$
通常取 \($\alpha = \beta = \gamma = 1$\)，简化后为：
$$SSIM(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}$$
其中：
*   \($\mu_x, \mu_y$\)：图像块 \($x$\) 和 \($y$\) 的均值，代表**亮度**的估计。
*   \($\sigma_x, \sigma_y$\)：图像块 \($x$\) 和 \($y$\) 的标准差，代表**对比度**的估计。
*   \($\sigma_{xy}$\)：图像块 \(x\) 和 \(y\) 的协方差，代表**结构**相似性的估计。
*   \($C_1, C_2$\)：是为了避免分母为零而设置的微小常数。
*   **亮度比较** \($l(x, y)$\)：比较平均灰度。
*   **对比度比较** \($c(x, y)$\)：比较标准差的比值。
*   **结构比较** \($s(x, y)$\)：比较协方差与标准差的乘积，实质上是去除了亮度和对比度影响后的相关系数。
#### 3. 解读与特点
*   **取值范围**：[-1, 1]。值为1表示两张图像完全相同；值越接近1，相似度越高。
*   **优点**：
    *   **与人类主观感知高度相关**，其性能远优于PSNR。
    *   考虑了图像的局部结构信息，更能反映人眼的感觉。
*   **缺点**：
    *   在某些情况下，如评估严重失真（如噪声、压缩伪影）或经过某些风格化处理的图像时，其表现仍不完美。
    *   仍然是一个**低层次**的相似性度量，无法理解图像的语义内容。
#### 4. 适用场景
*   目前**学术界和工业界最主流的传统图像质量评估指标**。
*   广泛应用于图像超分辨率、去噪、压缩、重建等任务的性能评估。
---
### 三、LPIPS（学习感知图像块相似度）
#### 1. 核心思想
LPIPS也被称为“**感知损失**”。它的核心思想是：**两张图像的差异，应该通过一个预先训练好的深度卷积神经网络（如VGG、AlexNet、SqueezeNet）来提取特征，然后在特征空间计算距离。** 它认为，深层网络的特征能够捕捉到图像的**高级语义信息**，这种差异更接近人类的判断。
#### 2. 计算方法
1.  **特征提取**：将原始图像和待评估图像同时输入一个预训练的网络（通常是在ImageNet上训练的分类网络）。
2.  **多层特征激活**：从网络的多个层级（通常是浅层、中层、深层）提取特征图。
3.  **通道归一化**：对每一层的特征图进行通道间的归一化处理，以减弱原始图像中亮度、颜色等无关信息的影响，聚焦于结构内容。
4.  **计算距离**：在每一层，计算两个图像特征图之间的\($L_2$\)距离。
5.  **加权求和**：将所有层的距离进行加权平均，得到最终的LPIPS值。
    *   权重是通过在人类评判的数据集（如BAIRE）上进行线性回归学习得到的，以确保最终分数与人类主观打分一致。
#### 3. 解读与特点
*   **取值范围**：理论上从0开始，值越小表示越相似。但没有上限，具体数值意义不如SSIM直观，更多用于相对比较。
*   **优点**：
    *   **与人类主观感知的相关性是目前所有指标中最高的**，尤其是在评估生成式模型（如GAN）、风格迁移等产生的图像时，表现远超PSNR和SSIM。
    *   能够理解**语义级别的差异**。例如，一张猫的图片和一张狗的图片，即使像素级相似，LPIPS值也会很高；而PSNR/SSIM可能因为背景相同而给出高值。
*   **缺点**：
    *   **计算成本最高**，需要前向传播通过一个深度网络。
    *   依赖于预训练模型，其性能受限于基础模型的质量和训练数据。
    *   结果不如PSNR/SSIM直观易懂。
#### 4. 适用场景
*   **生成对抗网络** 生成的图像质量评估（这是LPIPS最重要的应用场景）。
*   **图像风格迁移**、图像着色等语义内容可能发生变化的任务。
*   任何需要**高度符合人类视觉判断**的图像质量比较。
---
### 总结对比

| 指标 | 全称 | 核心原理 | 感知层面 | 优点 | 缺点 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **PSNR** | 峰值信噪比 | 像素级均方误差 | 信号保真度 | 计算简单，意义明确 | 与人类感知相关性差 | 图像压缩、初步快速评估 |
| **SSIM** | 结构相似性指数 | 亮度、对比度、结构的局部比较 | 低层次视觉感知 | 与感知相关度高，计算较快 | 对语义变化不敏感 | **主流**的图像重建、增强任务（超分、去噪等） |
| **LPIPS** | 学习感知图像块相似度 | 深度特征空间的距离 | **高层次语义感知** | **与人类判断最一致**，理解语义 | 计算复杂，依赖预训练模型 | **生成式模型**（GAN）、风格迁移、图像编辑 |


在选择指标时，应根据你的任务目标来决定：如果追求像素级的精确还原（如医学影像），PSNR仍有价值；如果关心视觉质量（如手机拍照优化），SSIM是更好的选择；如果图像的内容和风格发生了创造性变化（如AI绘画），LPIPS是黄金标准。在顶尖的研究中，通常会同时报告多个指标以提供更全面的评估。



当前先进的方法：
无监督学习：
- EnlightenGAN（JiangY,GongX,Liu D,ChengY,Fang C，Shen X,et al.Enlightengan: Deep light enhancement without paired super-vision. IEEETransactions onImageProcessing,202l，30:2340-2349）
- SCI（Toward Fast, Flexible, and Robust Low-Light Image Enhancement，https://doi.org/10.48550/arXiv.2204.10137


$$\begin{pmatrix}
1 & 2 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}=
\begin{pmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
 &  & 0
\end{pmatrix}
\begin{pmatrix}
1 & 2 & 0 \\
0 &  &  \\
0 & 0 & 
\end{pmatrix}$$